{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b2f01a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: (1000, 4)\n",
      "Missing values per column:\n",
      "median_house_value     0\n",
      "median_income         10\n",
      "housing_median_age     0\n",
      "total_rooms            0\n",
      "dtype: int64\n",
      "Income range: 0.00 to 11.39\n",
      "Bin edges: [1.18857025e-03 3.51408600e+00 4.66126623e+00 5.64622930e+00\n",
      " 6.85758054e+00 1.13862151e+01]\n",
      "Number of categories: 5\n",
      "\n",
      "Cleaned dataset shape: (990, 5)\n",
      "Income category distribution:\n",
      "income_cat\n",
      "1    198\n",
      "2    198\n",
      "3    198\n",
      "4    198\n",
      "5    198\n",
      "Name: count, dtype: int64\n",
      "\n",
      "============================================================\n",
      "STRATIFIED SHUFFLE SPLIT WITH MULTIPLE SPLITS\n",
      "============================================================\n",
      "\n",
      "Split 1:\n",
      "  Training set size: 792\n",
      "  Test set size: 198\n",
      "  Training income distribution: {1: 0.199, 2: 0.201, 3: 0.199, 4: 0.199, 5: 0.201}\n",
      "  Test income distribution: {1: 0.202, 2: 0.197, 3: 0.202, 4: 0.202, 5: 0.197}\n",
      "  RMSE: 105834.34\n",
      "\n",
      "Split 2:\n",
      "  Training set size: 792\n",
      "  Test set size: 198\n",
      "  Training income distribution: {1: 0.199, 2: 0.201, 3: 0.201, 4: 0.199, 5: 0.199}\n",
      "  Test income distribution: {1: 0.202, 2: 0.197, 3: 0.197, 4: 0.202, 5: 0.202}\n",
      "  RMSE: 101115.86\n",
      "\n",
      "Split 3:\n",
      "  Training set size: 792\n",
      "  Test set size: 198\n",
      "  Training income distribution: {1: 0.201, 2: 0.199, 3: 0.199, 4: 0.199, 5: 0.201}\n",
      "  Test income distribution: {1: 0.197, 2: 0.202, 3: 0.202, 4: 0.202, 5: 0.197}\n",
      "  RMSE: 100253.00\n",
      "\n",
      "Split 4:\n",
      "  Training set size: 792\n",
      "  Test set size: 198\n",
      "  Training income distribution: {1: 0.201, 2: 0.201, 3: 0.199, 4: 0.199, 5: 0.199}\n",
      "  Test income distribution: {1: 0.197, 2: 0.197, 3: 0.202, 4: 0.202, 5: 0.202}\n",
      "  RMSE: 94160.17\n",
      "\n",
      "Split 5:\n",
      "  Training set size: 792\n",
      "  Test set size: 198\n",
      "  Training income distribution: {1: 0.199, 2: 0.201, 3: 0.199, 4: 0.201, 5: 0.199}\n",
      "  Test income distribution: {1: 0.202, 2: 0.197, 3: 0.202, 4: 0.197, 5: 0.202}\n",
      "  RMSE: 95946.73\n",
      "\n",
      "============================================================\n",
      "SUMMARY STATISTICS:\n",
      "============================================================\n",
      "Mean RMSE: 99462.02\n",
      "Std RMSE:  4109.36\n",
      "Min RMSE:  94160.17\n",
      "Max RMSE:  105834.34\n",
      "95% Confidence Interval: [91407.67, 107516.37]\n",
      "\n",
      "============================================================\n",
      "WHY n_splits MATTERS:\n",
      "============================================================\n",
      "Instead of saying 'My model has X error', you can say:\n",
      "'My model has 99462 ± 4109 error'\n",
      "This gives you:\n",
      "  1. Confidence in your model's performance\n",
      "  2. Understanding of performance variability\n",
      "  3. Better comparison between different models\n",
      "  4. Detection of overfitting (high variance across splits)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# If you're working with real housing data, use this approach:\n",
    "def create_robust_income_categories(df, income_column='median_income'):\n",
    "    \"\"\"\n",
    "    Create income categories that handle edge cases properly\n",
    "    \"\"\"\n",
    "    # Remove rows with missing income data\n",
    "    df_clean = df.dropna(subset=[income_column]).copy()\n",
    "    \n",
    "    # Check the actual range of your data\n",
    "    print(f\"Income range: {df_clean[income_column].min():.2f} to {df_clean[income_column].max():.2f}\")\n",
    "    \n",
    "    # Create bins based on percentiles for more robust categorization\n",
    "    percentiles = [0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "    bin_edges = df_clean[income_column].quantile(percentiles).values\n",
    "    \n",
    "    # Ensure unique bin edges (in case of duplicate values)\n",
    "    bin_edges = np.unique(bin_edges)\n",
    "    \n",
    "    # Create labels\n",
    "    labels = list(range(1, len(bin_edges)))\n",
    "    \n",
    "    print(f\"Bin edges: {bin_edges}\")\n",
    "    print(f\"Number of categories: {len(labels)}\")\n",
    "    \n",
    "    # Create categories\n",
    "    df_clean['income_cat'] = pd.cut(df_clean[income_column], \n",
    "                                   bins=bin_edges, \n",
    "                                   labels=labels, \n",
    "                                   include_lowest=True)\n",
    "    \n",
    "    # Final check for NaN values\n",
    "    nan_count = df_clean['income_cat'].isna().sum()\n",
    "    if nan_count > 0:\n",
    "        print(f\"Warning: {nan_count} NaN values found. Removing them.\")\n",
    "        df_clean = df_clean.dropna(subset=['income_cat'])\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Example with synthetic data (you can replace this with your actual housing data)\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "# Create more realistic housing data\n",
    "housing_data = {\n",
    "    'median_house_value': np.random.normal(200000, 100000, n_samples),\n",
    "    'median_income': np.abs(np.random.normal(5, 2, n_samples)),  # Ensure positive values\n",
    "    'housing_median_age': np.random.randint(1, 50, n_samples),\n",
    "    'total_rooms': np.random.randint(1000, 8000, n_samples)\n",
    "}\n",
    "\n",
    "# Add a few missing values to simulate real data\n",
    "housing_data['median_income'][np.random.choice(n_samples, 10)] = np.nan\n",
    "\n",
    "housing = pd.DataFrame(housing_data)\n",
    "\n",
    "print(\"Original dataset shape:\", housing.shape)\n",
    "print(\"Missing values per column:\")\n",
    "print(housing.isnull().sum())\n",
    "\n",
    "# Clean the data and create categories\n",
    "housing_clean = create_robust_income_categories(housing)\n",
    "\n",
    "print(f\"\\nCleaned dataset shape: {housing_clean.shape}\")\n",
    "print(\"Income category distribution:\")\n",
    "print(housing_clean['income_cat'].value_counts().sort_index())\n",
    "\n",
    "# Now use StratifiedShuffleSplit\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STRATIFIED SHUFFLE SPLIT WITH MULTIPLE SPLITS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "splitter = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "rmse_scores = []\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(splitter.split(housing_clean, housing_clean['income_cat'])):\n",
    "    train_set = housing_clean.iloc[train_index]\n",
    "    test_set = housing_clean.iloc[test_index]\n",
    "    \n",
    "    # Verify stratification worked\n",
    "    print(f\"\\nSplit {i+1}:\")\n",
    "    print(f\"  Training set size: {len(train_set)}\")\n",
    "    print(f\"  Test set size: {len(test_set)}\")\n",
    "    print(f\"  Training income distribution: {train_set['income_cat'].value_counts(normalize=True).sort_index().round(3).to_dict()}\")\n",
    "    print(f\"  Test income distribution: {test_set['income_cat'].value_counts(normalize=True).sort_index().round(3).to_dict()}\")\n",
    "    \n",
    "    # Train a simple model\n",
    "    feature_columns = ['median_income', 'housing_median_age', 'total_rooms']\n",
    "    X_train = train_set[feature_columns]\n",
    "    y_train = train_set['median_house_value']\n",
    "    X_test = test_set[feature_columns]\n",
    "    y_test = test_set['median_house_value']\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "    rmse_scores.append(rmse)\n",
    "    \n",
    "    print(f\"  RMSE: {rmse:.2f}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY STATISTICS:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Mean RMSE: {np.mean(rmse_scores):.2f}\")\n",
    "print(f\"Std RMSE:  {np.std(rmse_scores):.2f}\")\n",
    "print(f\"Min RMSE:  {np.min(rmse_scores):.2f}\")\n",
    "print(f\"Max RMSE:  {np.max(rmse_scores):.2f}\")\n",
    "print(f\"95% Confidence Interval: [{np.mean(rmse_scores) - 1.96*np.std(rmse_scores):.2f}, {np.mean(rmse_scores) + 1.96*np.std(rmse_scores):.2f}]\")\n",
    "\n",
    "# Show why n_splits matters\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"WHY n_splits MATTERS:\")\n",
    "print(\"=\"*60)\n",
    "print(\"Instead of saying 'My model has X error', you can say:\")\n",
    "print(f\"'My model has {np.mean(rmse_scores):.0f} ± {np.std(rmse_scores):.0f} error'\")\n",
    "print(\"This gives you:\")\n",
    "print(\"  1. Confidence in your model's performance\")\n",
    "print(\"  2. Understanding of performance variability\")\n",
    "print(\"  3. Better comparison between different models\")\n",
    "print(\"  4. Detection of overfitting (high variance across splits)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
